import concurrent.futures
import os
import argparse
import pickle
from datetime import datetime
from typing import Type

import numpy as np
import pandas as pd

from anomaly_detector import (
    AbstractAnomalyDetector,
    ANOMALY_DETECTOR_CLASSES
)

"""
True  Positive (TP) - actual condition is positive AND assigned test outcome is positive
False Negative (FN) - actual condition is positive AND assigned test outcome is negative
True  Negative (TN) - actual condition is negative AND assigned test outcome is negative
False Positive (FP) - actual condition is negative AND assigned test outcome is positive

Total number of all actual positives = TP + FN
Total number of all actual negatives = TN + FP

True Positive Rate (TPR) = (TP/(TP+FN)), aka sensitivity or recall.
False Negative Rate (FNR) = (FN/(TP+FN)) = (1 - TPR)
True Negative Rate (TNR) = (TN/(TN+FP)), aka specificity (SPC).
False Positive Rate (FPR) = (FP/(TN+FP)) = (1 - TNR)

The ROC curve is the plot of the true positive rate (TPR) against 
the false positive rate (FPR) at each threshold setting.

In the case of biometric authentication, the hypothesis is:
"Is the given data sample anomalous, thus is the current user an impostor?"

Then the "Positive" answer corresponds to the anomaly being present and the current user being an impostor,
and the "Negative" answer corresponds to the anomaly being absent and the current user being a genuine user.

Hit Rate - how often the anomalies are detected correctly; is equal to TPR.
Miss Rate = (1 - Hit Rate); how often the anomalies are missed; is equal to FNR.

False-Alarm Rate (FAR) - how often the anomalies are actually absent 
but detected; is equal to FPR.

Zero-Miss False-Alarm Rate (ZMFAR) - is the minimum FAR value given that Miss Rate = 0.

Equal Error Rate (EER) - is equal to both Miss Rate and FAR when 
the threshold value is picked in such a way that (Miss Rate = FAR).

An anomaly detector outputs an anomaly score for a given typing sample.
The higher the anomaly score, the higher the probability of the sample to be generated by an impostor.
"""

parser = argparse.ArgumentParser()
parser.add_argument("--detector")
parser.add_argument("--exclude_dd_features", type=bool)
parser.add_argument("--exclude_ud_features", type=bool)
parser.add_argument("--exclude_enter_features", type=bool)
parser.add_argument("--use_sliding_window", type=bool)
parser.add_argument("--impostors_practice", type=bool)
args = parser.parse_args()

detector_name: str = args.detector or 'manhattan'
exclude_dd_features: bool = args.exclude_dd_features or False
exclude_ud_features: bool = args.exclude_ud_features or False
exclude_enter_features: bool = args.exclude_enter_features or False
use_sliding_window: bool = args.use_sliding_window or False
impostors_practice: bool = args.impostors_practice or False

print(
    detector_name,
    'exclude_dd_features' * exclude_dd_features,
    'exclude_ud_features' * exclude_ud_features,
    'exclude_enter_features' * exclude_enter_features,
    'use_sliding_window' * use_sliding_window,
    'impostors_practice' * impostors_practice
)

detector_class: Type[AbstractAnomalyDetector] = ANOMALY_DETECTOR_CLASSES[detector_name]

output_dir_name = 'output'
detector_dir_path = f'{output_dir_name}/{detector_name}'

if exclude_dd_features:
    detector_dir_path += "_no_dd"
if exclude_ud_features:
    detector_dir_path += "_no_ud"
if exclude_enter_features:
    detector_dir_path += "_no_enter"
if use_sliding_window:
    detector_dir_path += "_updating"
if impostors_practice:
    detector_dir_path += "_practice"

samples = pd.read_csv('DSL-StrongPasswordData.csv')

if exclude_dd_features:
    samples = samples.loc[:, ~samples.columns.str.startswith('DD')]
if exclude_ud_features:
    samples = samples.loc[:, ~samples.columns.str.startswith('UD')]
if exclude_enter_features:
    samples = samples.loc[:, ~samples.columns.str.endswith('.Return')]

subject_ids: np.ndarray = samples['subject'].unique()
feature_columns_slice = slice(3, None)
# Take either 5 first or 5 last samples from each subject.
common_pos_test_data = samples.groupby('subject')
if impostors_practice:
    common_pos_test_data = common_pos_test_data.tail(5)
else:
    common_pos_test_data = common_pos_test_data.head(5)

stats = []

start_time = datetime.now()
print(f"start_time: {start_time}")

# for training_data_size in [
#     # 5
#     # 100
#     *range(5, 101, 5),
#     # *range(60, 101, 10),
#     # *range(120, 201, 20)
# ]:
def process_training_data(training_data_size, detector_dir_path, subject_ids, samples, common_pos_test_data,
                              feature_columns_slice, use_sliding_window, detector_class):
    print(f'training_data_size: {training_data_size}')
    iteration_start_time = datetime.now()
    print(f'iteration_start_time: {iteration_start_time}')
    equal_error_rates = np.array([])
    zero_miss_false_alarm_rates = np.array([])

    training_size_dir_path = f'{detector_dir_path}/train_{training_data_size}'
    os.makedirs(training_size_dir_path, exist_ok=True)

    for subject_id in subject_ids:
        print(f'{training_data_size}:{subject_id}', end=' ', flush=True)
        curr_subject_samples = samples[samples['subject'] == subject_id]

        # first 5 password repetitions from each of the remaining 50 users, 250 in total
        pos_test_data = common_pos_test_data[common_pos_test_data['subject'] != subject_id]\
            .iloc[:, feature_columns_slice]

        if not use_sliding_window:
            # first `training_data_size` password repetitions of the current genuine user
            train_data = curr_subject_samples\
                 .iloc[:training_data_size]\
                 .iloc[:, feature_columns_slice]
            # subsequent 200 password repetitions of the current genuine user
            neg_test_data = curr_subject_samples\
                .iloc[training_data_size:training_data_size + 200]\
                .iloc[:, feature_columns_slice]

            ad = detector_class()
            ad.train(train_data)
            # Score the genuine user's samples
            neg_test_data_scores = ad.score(neg_test_data)
            # Score the impostors' samples
            pos_test_data_scores = ad.score(pos_test_data)
        else:
            neg_test_data_scores = pd.DataFrame()
            pos_test_data_scores = pd.DataFrame()
            # The sliding window is being advanced in increments of 5 repetitions
            # for computational efficiency and in the same way it's done in the
            # evaluation procedure described by Killourhy and Maxion.
            for i in range(0, 200, 5):
                # first `training_data_size` password repetitions of the current genuine user
                train_data = curr_subject_samples.iloc[i:i + training_data_size]\
                     .iloc[:, feature_columns_slice]
                # subsequent 5 password repetitions of the current genuine user
                neg_test_data = curr_subject_samples.iloc[i + training_data_size:i + training_data_size + 5]\
                    .iloc[:,feature_columns_slice]
                ad = detector_class()
                ad.train(train_data)
                # Append new 5 genuine-user scores
                neg_test_data_scores = pd.concat(
                    [neg_test_data_scores, ad.score(neg_test_data)],
                    ignore_index=True
                )
                # Append new 250 impostor-user scores
                pos_test_data_scores = pd.concat(
                    [pos_test_data_scores, ad.score(pos_test_data)],
                    ignore_index=True
                )

        # The arrays below will store the numbers of corresponding outcomes for
        # each threshold value.
        true_positive = np.array([])
        false_negative = np.array([])
        true_negative = np.array([])
        false_positive = np.array([])

        thresholds_df = pd.concat([neg_test_data_scores, pos_test_data_scores]).sort_values(by='score')
        # Append a threshold value that is larger than all outcomes to achieve
        # edge cases of FNR = 1 and TNR = 1
        thresholds_df = pd.concat(
            [
                thresholds_df,
                pd.DataFrame({'score': [thresholds_df['score'].iloc[-1] + 1]})
            ],
            ignore_index=True
        )

        for _, row in thresholds_df.iterrows():
            threshold = row['score']
            # Given such comparison, there is at least one threshold, so that Hit Rate = 1 (required for ZMFAR)
            curr_true_positives_num = pos_test_data_scores[pos_test_data_scores['score'] >= threshold].shape[0]
            true_positive = np.append(true_positive, curr_true_positives_num)
            false_negative = np.append(false_negative, pos_test_data_scores.shape[0] - curr_true_positives_num)

            curr_false_positives_num = neg_test_data_scores[neg_test_data_scores['score'] >= threshold].shape[0]
            false_positive = np.append(false_positive, curr_false_positives_num)
            true_negative = np.append(true_negative, neg_test_data_scores.shape[0] - curr_false_positives_num)

        true_positive_rate = true_positive / (true_positive + false_negative)
        false_positive_rate = false_positive / (false_positive + true_negative)

        # Find the index of the elements when the difference between (1 - TPR) and FPR
        # is minimal, so that (1 - TPR) = FPR at least approximately.
        equal_error_rate_idx = np.argmin(np.abs((1 - true_positive_rate) - false_positive_rate))
        # Find the average of the found values and store it as EER.
        equal_error_rate = ((1 - true_positive_rate[equal_error_rate_idx]) + false_positive_rate[equal_error_rate_idx]) / 2

        # Zero-miss false-alarm rate
        zmfar = np.min(false_positive_rate[true_positive_rate == 1])
        zmfar_idx = np.where(false_positive_rate == zmfar)[0][0]

        roc_curve_data_file_path = f'{training_size_dir_path}/roc_{subject_id}.pkl'
        with open(roc_curve_data_file_path, 'wb') as f:
            pickle.dump(
                (
                    true_positive_rate,
                    false_positive_rate,
                    zmfar_idx,
                    equal_error_rate_idx
                ),
                f
            )

        equal_error_rates = np.append(equal_error_rates, equal_error_rate)
        zero_miss_false_alarm_rates = np.append(zero_miss_false_alarm_rates, zmfar)
    print()

    print(equal_error_rates)
    print(zero_miss_false_alarm_rates)

    err_hist_data_file_path = f'{training_size_dir_path}/eer_hist.pkl'
    with open(err_hist_data_file_path, 'wb') as f:
        pickle.dump(equal_error_rates, f)

    zmfar_hist_data_file_path = f'{training_size_dir_path}/zmfar_hist.pkl'
    with open(zmfar_hist_data_file_path, 'wb') as f:
        pickle.dump(zero_miss_false_alarm_rates, f)

    new_stats_dict = {
        'trainset_size': training_data_size,
        'eer_avg': np.mean(equal_error_rates),
        'eer_std': np.std(equal_error_rates),
        'zmfar_avg': np.mean(zero_miss_false_alarm_rates),
        'zmfar_std': np.std(zero_miss_false_alarm_rates)
    }
    print(new_stats_dict)
    # stats.append(new_stats_dict)
    iteration_end_time = datetime.now()
    print(f'iteration_end_time: {iteration_end_time}')
    print(f'Elapsed time: {iteration_end_time - iteration_start_time}')
    return new_stats_dict

training_data_sizes = [*range(40, 101, 5)]

with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:
    # Submit all tasks to the pool
    futures = [
        executor.submit(
            process_training_data,
            training_data_size,
            detector_dir_path,
            subject_ids,
            samples,
            common_pos_test_data,
            feature_columns_slice,
            use_sliding_window,
            detector_class
        )
        for training_data_size in training_data_sizes
    ]

    # Collect the results as they complete
    for future in concurrent.futures.as_completed(futures):
        stats.append(future.result())

stats = pd.DataFrame(stats)
stats.to_csv(f'{detector_dir_path}/stats.csv', index=False)

end_time = datetime.now()
print(f'end_time: {end_time}')
print(f'Elapsed time: {end_time - start_time}')
